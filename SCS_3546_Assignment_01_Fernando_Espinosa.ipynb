{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fernandoespinosa/3253-machine-learning-team-project/blob/main/SCS_3546_Assignment_01_Fernando_Espinosa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6OQBFpIHP30"
      },
      "source": [
        "# SCS 3546: Deep Learning\n",
        "> Assignment 1: Deep Learning Using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzRpIFXzolmz"
      },
      "source": [
        "### Your name & student number:\n",
        "\n",
        "<pre> Please enter your name here. </pre>\n",
        "\n",
        "<pre> Please enter your student number here. </pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aHEi-CRlKBb"
      },
      "source": [
        "## Assignment Description\n",
        "\n",
        "In this assignment you will demonstrate your ability to:\n",
        "\n",
        "- Train a neural network using Keras to solve a regression problem.\n",
        "\n",
        "- Perform sensible data preprocessing.\n",
        "\n",
        "- Experiment with hyperparemter tuning and different model architectures to achieve best performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OKLV1eJ12dy"
      },
      "source": [
        "### Grade Allocation\n",
        "\n",
        "**15 points total**\n",
        "\n",
        "- Part 1: 4 Marks\n",
        "- Part 2: 9 Marks\n",
        "- Clarity: 2 Marks\n",
        "\n",
        "The marks for clarity are awarded for code documentation and how well you explained/supported your answers, including the use of visualizations where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL : For model evaluation and for those who choose to use the KerasRegressor from keras.wrappers, you would need to install tensorflow 2.12 or 2.11.another option is to use the scikeras.\n",
        "# please note that you may have many options and the below libraries are just meant to help you and to provide options\n",
        "\n",
        "# !pip install tensorflow==2.12\n",
        "# from keras.wrappers.scikit_learn import KerasRegressor\n",
        "\n",
        "!pip install scikeras\n",
        "from scikeras.wrappers import KerasClassifier, KerasRegressor"
      ],
      "metadata": {
        "id": "LUd8H6Ji_w16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruXPP1zUT_B6"
      },
      "outputs": [],
      "source": [
        "# setting up the notebook with important libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn-eYs8MfGAQ"
      },
      "source": [
        "# Preamble\n",
        "\n",
        "### Hyperparameters\n",
        "\n",
        "A hyperparameter is a parameter whose value is set before the learning process begins.\n",
        "\n",
        "Some important Neural Networks hyperparameters include:\n",
        "\n",
        "- number of hidden layers\n",
        "- number of neurons\n",
        "- learning rate\n",
        "- activation function\n",
        "- optimizer settings\n",
        "\n",
        "Hyperparameters are crucial to the performance, speed, and quality of the machine learning models.\n",
        "\n",
        "Through Hyper parameter optimization, we find a tuple (best combination) of hyperparameters that yields an optimal model which minimizes a predefined loss function on given test data.\n",
        "\n",
        "Important hyperparameters that could be tuned include:\n",
        "\n",
        "- num_hidden_layers\n",
        "- neurons_per_layer\n",
        "- dropout_rate\n",
        "- activation\n",
        "- optimizer\n",
        "- learning_rate\n",
        "- batch_size\n",
        "\n",
        "### Loss Function\n",
        "\n",
        "- MSE (Mean Squared Error) is used as the score/loss function that will be minimized for hyperparameter optimization.\n",
        "- In this assignment, we are going to use Cross-Validation to calculate the score (MSE) for a given set of hyperparameter values\n",
        "\n",
        "- MSE is a desirable metric because by taking the square root gives us an error value we can directly understand in the context of the problem; for example, in this assignment it translates to thousands of dollars\n",
        "\n",
        "- Note: Your results may vary given the stochastic nature of the algorithm, evaluation procedure, or differences in numerical precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs0FlcDeaSpi"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We will be using mock data for this assignment. the outcome is the sale price for some products. we have created 13 features to predict the sale price.\n",
        "\n",
        "- import provided mock data into your notebook.\n",
        "\n",
        "- You are **not** expected to perform Exploratory Data Analysis (EDA) on this dataset.\n",
        "\n",
        "- For the purpose of this assignment, your model's performance is not an important factor by itself - that is how far your sale price prediction is. The important factor is the changes that you observe when changing model architectures.\n",
        "\n",
        "- The information that follow are meant to be optional and to help you get familiar with the data. Your efforts on this assignment should focus on **model training and hyperparameter tuning**, not on EDA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHL_GkQPoJE9"
      },
      "source": [
        "# Assignment Start\n",
        "***\n",
        "\n",
        "- Please follow all instructions carefully.\n",
        "\n",
        "- Use MSE (Mean Squared Error) as the score/loss function that will be minimized during optimization.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "50_-6mCmWNHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar6TpPmfeoDy"
      },
      "source": [
        "#Data Import\n",
        "\n",
        "The code below imports the data for you as dataframe, then you may need to convert it to numpy arrays.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can use the code below to import the data\n",
        "# Option 3 - from local using the files that are available\n",
        "import pandas as pd\n",
        "\n",
        "# mount the colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Next, You need to upload tables to your G drive -  then you may need to update  paths below\n",
        "# load the  data into a pandas dataframe for easy viewing and manipulation\n",
        "base_folder = '/content/drive/My Drive/Colab Notebooks/SCS-3546-034-Deep-Learning/Assignment-1-Deep-Learning-Using-Keras'\n",
        "df_train_y = pd.read_csv(f\"{base_folder}/y_train.csv\")\n",
        "df_test_y = pd.read_csv(f\"{base_folder}/y_test.csv\")\n",
        "df_train_x  =  pd.read_csv(f\"{base_folder}/X_train.csv\")\n",
        "df_test_x =  pd.read_csv(f\"{base_folder}/X_test.csv\")"
      ],
      "metadata": {
        "id": "DuK-rb0uwqw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'df_train_x.shape: {df_train_x.shape}')\n",
        "print(f'df_train_y.shape: {df_train_y.shape}')\n",
        "print(f'df_test_x.shape: {df_test_x.shape}')\n",
        "print(f'df_test_y.shape: {df_test_y.shape}')"
      ],
      "metadata": {
        "id": "MPICDeHjy-FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train_x.info())\n",
        "print(df_train_y.info())\n",
        "print(df_test_x.info())\n",
        "print(df_test_y.info())\n"
      ],
      "metadata": {
        "id": "1uTdOkeZDNL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the first 'Unnamed: 0' column from all dataframes\n",
        "X_train = df_train_x.drop('Unnamed: 0', axis=1)\n",
        "y_train = df_train_y.drop('Unnamed: 0', axis=1)\n",
        "\n",
        "X_test = df_test_x.drop('Unnamed: 0', axis=1)\n",
        "y_test = df_test_y.drop('Unnamed: 0', axis=1)\n",
        "\n",
        "print(f'X_train.shape: {X_train.shape}')\n",
        "print(f'y_train.shape: {y_train.shape}')\n",
        "\n",
        "print(f'X_test.shape: {X_test.shape}')\n",
        "print(f'y_test.shape: {y_test.shape}')"
      ],
      "metadata": {
        "id": "ufQgWmllE_cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hint : to make it easier, you may try to standardize your data upfront\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train) # NOTE: no need to do `scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()`\n",
        "\n",
        "# transform the test data using the same scalers but don't fit again!\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "y_test_scaled = scaler_y.transform(y_test)\n"
      ],
      "metadata": {
        "id": "6PgAwae1wxpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "SEED_MAGIC_NUMBER = 42\n",
        "\n",
        "np.random.seed(SEED_MAGIC_NUMBER)\n",
        "tf.random.set_seed(SEED_MAGIC_NUMBER)"
      ],
      "metadata": {
        "id": "0LaeTr_pyi9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Unfortunately, using cross_val_score directly with KerasRegressor doesn't allow easy\n",
        "# access to the history object for plotting learning curves. Therefore, we will train\n",
        "# the model within the KFold loop manually, record the learning curves for each fold,\n",
        "# and plot them at the end.\n",
        "#\n",
        "def kfold_cv(model, X, y, epochs, n_splits):\n",
        "  keras_regressor = KerasRegressor(model=model, epochs=epochs, batch_size=5, verbose=0)\n",
        "  pipeline = Pipeline([\n",
        "      ('scaler', StandardScaler()),\n",
        "      ('regressor', keras_regressor)\n",
        "  ])\n",
        "\n",
        "  # kfold = KFold(n_splits=n_splits)\n",
        "  kfold = KFold(n_splits=n_splits, shuffle=True, random_state=SEED_MAGIC_NUMBER)\n",
        "  score = cross_val_score(pipeline, X, y, cv=kfold, scoring='neg_mean_squared_error')\n",
        "  print_score_summary(score, model, epochs, n_splits)\n",
        "  return score\n",
        "\n",
        "def print_score_summary(score, model, epochs, n_splits):\n",
        "  print(f\"{model.__name__} score ({epochs} epochs – {n_splits} splits): {score}\")\n",
        "  print(f\"\\t µ: {score.mean():.2f}\")\n",
        "  print(f\"\\t σ: {score.std():.3f}\")"
      ],
      "metadata": {
        "id": "aZZPM2O9z6wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Train the model within the KFold loop manually and record the learning curves\n",
        "# for each fold to be plotted later.\n",
        "#\n",
        "# Also, report MAE mean and standard deviation across all folds\n",
        "#\n",
        "# X and y should already scaled\n",
        "#\n",
        "def kfold_history_score(model, X, y, epochs, n_splits):\n",
        "  kfold = KFold(n_splits=n_splits, shuffle=True, random_state=SEED_MAGIC_NUMBER)\n",
        "\n",
        "  # training history for later plotting\n",
        "  history = []\n",
        "  mae_history = []\n",
        "\n",
        "  # KFold cross-validation loop\n",
        "  n = 0\n",
        "  for train_index, val_index in kfold.split(X):\n",
        "    # split\n",
        "    X_train_, X_val_ = X[train_index], X[val_index]\n",
        "    y_train_, y_val_ = y[train_index], y[val_index]\n",
        "\n",
        "    model_ = model()\n",
        "\n",
        "    # train the model with current fold\n",
        "    res = model_.fit(\n",
        "        X_train_,\n",
        "        y_train_,\n",
        "        validation_data=(X_val_, y_val_),\n",
        "        epochs=epochs,\n",
        "        batch_size=10,\n",
        "        verbose=0)\n",
        "\n",
        "    history.append(res.history)\n",
        "\n",
        "    # evaluate model\n",
        "    score = model_.evaluate(X_val_, y_val_, verbose=0)\n",
        "    mae_history.append(score[1])\n",
        "\n",
        "    print(f'{model.__name__} – fold {n} – MAE: {score[1]}')\n",
        "\n",
        "    n += 1\n",
        "\n",
        "  print()\n",
        "\n",
        "  mae_mean = np.mean(mae_history)\n",
        "  mae_std = np.std(mae_history)\n",
        "\n",
        "  print(f'validation loss (MSE) µ across all folds: {mae_mean}')\n",
        "  print(f'validation loss (MSE) σ across all folds: {mae_std}')\n",
        "\n",
        "  return history\n"
      ],
      "metadata": {
        "id": "zllN9YDtz8xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curves(history):\n",
        "  plt.figure(figsize=(14, 10))\n",
        "\n",
        "  num_epochs = len(history[0]['loss'])  # Assuming all folds have the same number of epochs\n",
        "\n",
        "  # calculate average metrics across all folds\n",
        "  loss_train_avg = np.mean([h['loss'] for h in history], axis=0)\n",
        "  loss_train_std = np.std([h['loss'] for h in history], axis=0)\n",
        "\n",
        "  loss_val_avg = np.mean([h['val_loss'] for h in history], axis=0)\n",
        "  loss_val_std = np.std([h['val_loss'] for h in history], axis=0)\n",
        "\n",
        "  mae_train_avg = np.mean([h['mae'] for h in history], axis=0)\n",
        "  mae_train_std = np.std([h['mae'] for h in history], axis=0)\n",
        "\n",
        "  mae_val_avg = np.mean([h['val_mae'] for h in history], axis=0)\n",
        "  mae_val_std = np.std([h['val_mae'] for h in history], axis=0)\n",
        "\n",
        "  epochs = range(1, num_epochs + 1)\n",
        "\n",
        "  # subplot 1: loss (MSE) over epochs\n",
        "  plt.subplot(2, 1, 1)\n",
        "  plt.plot(epochs, loss_train_avg, label='Training Loss (MSE)')\n",
        "  plt.plot(epochs, loss_val_avg, label='Validation Loss (MSE)')\n",
        "  plt.title('Training and Validation Loss (MSE) over epochs')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss (MSE)')\n",
        "  plt.legend()\n",
        "\n",
        "  # subplot 2: MAE over epochs\n",
        "  plt.subplot(2, 1, 2)\n",
        "  plt.plot(epochs, mae_train_avg, label='Training MAE')\n",
        "  plt.plot(epochs, mae_val_avg, label='Validation MAE')\n",
        "  plt.title('Training and Validation Mean Absolute Error (MAE) over epochs')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Mean Absolute Error (MAE)')\n",
        "  plt.legend()\n",
        "\n",
        "  # Show the plots\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "Y-Kst20NIomI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDl1bkKjbbGw"
      },
      "source": [
        "# Part 1: Impact of Changing Model Architecture\n",
        "\n",
        "In this section, we will be comparing a simple single-layer baseline model with two other models having a different network topology."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z82OOouVaVrt"
      },
      "source": [
        "## a) Baseline model [2 points]\n",
        "\n",
        "Use Keras to develop a baseline neural network model that has **one single fully-connected hidden layer with the same number of neurons as input features (i.e. 13 neurons).**\n",
        "\n",
        "Make sure to **standardize** your features (i.e. subtract mean and divide by standard deviation) before training your model. You can also perform any other data-preprocessing that you deem necessary.\n",
        "\n",
        "- Note: No activation function is used for the output layer because it is a regression problem and we are interested in predicting numerical values directly without transformation.\n",
        "\n",
        "- The ADAM optimization algorithm should be used to optimize mean squared error loss function.\n",
        "\n",
        "- Plot learning curves and report on both training and validation performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def baseline_neural_network():\n",
        "  model = keras.models.Sequential()\n",
        "  input = keras.layers.Input(shape=(13,))\n",
        "  model.add(input)\n",
        "  model.add(keras.layers.Dense(13, activation='relu'))\n",
        "  model.add(keras.layers.Dense(1))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "GDTmReBCa0bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `epochs=20, n_splits=5`"
      ],
      "metadata": {
        "id": "Y-reQPPXRBY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_score_baseline_20_5 = kfold_history_score(baseline_neural_network, X_train_scaled, y_train_scaled, epochs=20, n_splits=5)\n"
      ],
      "metadata": {
        "id": "P33Z-sRWBSOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history_score_baseline_20_5)\n"
      ],
      "metadata": {
        "id": "Jwfilh4KpAS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `epochs=20, n_splits=10`"
      ],
      "metadata": {
        "id": "3eeHDLUwQ8K6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_score_baseline_20_10 = kfold_history_score(baseline_neural_network, X_train_scaled, y_train_scaled, epochs=20, n_splits=10)\n"
      ],
      "metadata": {
        "id": "RsGSwgc-a1kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history_score_baseline_20_10)\n"
      ],
      "metadata": {
        "id": "FGV1Q8MToxYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JimscX-Zb7T"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## b) Deeper Network [1 point]\n",
        "\n",
        "Construct and evaluate a model with 2 dense layers having a smaller number of neurons (e.g. 16, 8)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha6iU4_WKjJE"
      },
      "outputs": [],
      "source": [
        "def deep_neural_network():\n",
        "  model = keras.models.Sequential()\n",
        "  input = keras.layers.Input(shape=(13,))\n",
        "  model.add(input)\n",
        "  model.add(keras.layers.Dense(13, activation='relu'))\n",
        "  model.add(keras.layers.Dense(8, activation='relu'))\n",
        "  model.add(keras.layers.Dense(1))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `epochs=20, n_splits=5`"
      ],
      "metadata": {
        "id": "sEzbY7n_SE9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_score_deep_20_5 = kfold_history_score(deep_neural_network, X_train_scaled, y_train_scaled, epochs=20, n_splits=5)\n"
      ],
      "metadata": {
        "id": "a1nXjGvFom3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history_score_deep_20_5)\n"
      ],
      "metadata": {
        "id": "tGc2FgaYStLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `epochs=20, n_splits=10`"
      ],
      "metadata": {
        "id": "aE5uHkO1SGck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_score_deep_20_10 = kfold_history_score(deep_neural_network, X_train_scaled, y_train_scaled, epochs=20, n_splits=10)\n"
      ],
      "metadata": {
        "id": "i9o8jjaMohcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history_score_deep_20_10)\n"
      ],
      "metadata": {
        "id": "v_8-QkKGSuuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f0v_F7DZoAx"
      },
      "source": [
        "## c) Wider Network [1 point]\n",
        "\n",
        "Construct and evaluate a wider model with more neurons (e.g. 32, 16)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9W6c2mxKhWA"
      },
      "outputs": [],
      "source": [
        "def wide_neural_network():\n",
        "  model = keras.models.Sequential()\n",
        "  input = keras.layers.Input(shape=(13,))\n",
        "  model.add(input)\n",
        "  model.add(keras.layers.Dense(32, activation='relu'))\n",
        "  model.add(keras.layers.Dense(16, activation='relu'))\n",
        "  model.add(keras.layers.Dense(1))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `epochs=20, n_splits=5`"
      ],
      "metadata": {
        "id": "eTYwX09eSI51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_score_wide_20_5 = kfold_history_score(wide_neural_network, X_train_scaled, y_train_scaled, epochs=20, n_splits=5)\n"
      ],
      "metadata": {
        "id": "VYz3RgAsoCFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history_score_wide_20_5)\n"
      ],
      "metadata": {
        "id": "i6sbAUyLS3vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `epochs=20, n_splits=10`"
      ],
      "metadata": {
        "id": "_XKKx5bBSJh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_score_wide_20_10 = kfold_history_score(wide_neural_network, X_train_scaled, y_train_scaled, epochs=20, n_splits=10)\n"
      ],
      "metadata": {
        "id": "r3hglGoKn85c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history_score_wide_20_10)\n"
      ],
      "metadata": {
        "id": "LKg6nfWuS6Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I-2_jUCNThp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfdE-QPBrOpl"
      },
      "source": [
        "# Part 2: Hyperparameter Tuning Experiments\n",
        "\n",
        "In the following experiments, you will evaluate and compare models trained with different hyperparameters. Please follow the specifications given for each model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXsONE-iwyEZ"
      },
      "source": [
        "## a) Model 1 [2 points]\n",
        "\n",
        "- 2 Dense layers:\n",
        "  - The first with 64 neurons using a ReLU activation function.\n",
        "  - The second with 64 neurons using a ReLU activation function.\n",
        "- Choose an appropriate output layer and activation.\n",
        "- Train model with 100 epochs and obtain cross-validated performance (e.g. with 3 cross-folds).\n",
        "- Plot both loss and mean absolute error (i.e. learning curves) for both training and validation.\n",
        "- Report MAE from CV with standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dE-0BhtjM6Fn"
      },
      "outputs": [],
      "source": [
        "def model_1():\n",
        "  model = keras.models.Sequential()\n",
        "  input = keras.layers.Input(shape=(13,))\n",
        "  model.add(input)\n",
        "  model.add(keras.layers.Dense(64, activation='relu'))\n",
        "  model.add(keras.layers.Dense(64, activation='relu'))\n",
        "  # NOTE: No activation function is used for the output layer because this is a regression,\n",
        "  # and we are interested in predicting numerical values directly without transformation\n",
        "  model.add(keras.layers.Dense(1))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_score_model_1 = kfold_history_score(model_1, X_train_scaled, y_train_scaled, epochs=100, n_splits=3)\n"
      ],
      "metadata": {
        "id": "Y6BL5aMRncUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history_score_model_1)\n"
      ],
      "metadata": {
        "id": "tbFKND-_xm60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_score_model_1_5_splits = kfold_history_score(model_1, X_train_scaled, y_train_scaled, epochs=100, n_splits=5)\n"
      ],
      "metadata": {
        "id": "3fFjcsL7Rokj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history_score_model_1_5_splits)\n"
      ],
      "metadata": {
        "id": "YSI0pucaRqZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYsP54_x2ixg"
      },
      "source": [
        "## b) Model 2 [2 points]\n",
        "\n",
        "- 2 Dense layers:\n",
        "  - The first with 128 neurons using a ReLU activation function.\n",
        "  - The second with 64 neurons using a ReLU activation function.\n",
        "- Choose an appropriate output layer and activation.\n",
        "- Train model with 100 epochs and obtain cross-validated performance (e.g. with 3 cross-folds).\n",
        "- Plot both loss and mean absolute error (i.e. learning curves) for both training and validation.\n",
        "- Report MAE from CV with standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XklV0uGENLsw"
      },
      "outputs": [],
      "source": [
        "def model_2():\n",
        "  model = keras.models.Sequential()\n",
        "  input = keras.layers.Input(shape=(13,))\n",
        "  model.add(input)\n",
        "  model.add(keras.layers.Dense(128, activation='relu'))\n",
        "  model.add(keras.layers.Dense(64, activation='relu'))\n",
        "  # NOTE: No activation function is used for the output layer because this is a regression,\n",
        "  # and we are interested in predicting numerical values directly without transformation\n",
        "  model.add(keras.layers.Dense(1))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_score_model_2 = kfold_history_score(model_2, X_train_scaled, y_train_scaled, epochs=100, n_splits=3)\n"
      ],
      "metadata": {
        "id": "gtwgKe4YnYLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history_score_model_2)\n"
      ],
      "metadata": {
        "id": "hnKZVLLM3iGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_score_model_2_5_splits = kfold_history_score(model_1, X_train_scaled, y_train_scaled, epochs=100, n_splits=5)\n"
      ],
      "metadata": {
        "id": "c8u5jRUmR1n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history_score_model_2_5_splits)\n"
      ],
      "metadata": {
        "id": "F7IJ3eJVR7dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30A0lCy74RKe"
      },
      "source": [
        "## c) Model 3 [2 points]\n",
        "\n",
        "- Same as Model 2, but use tanh activation functions instead of relu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNpDFL2FNi1C"
      },
      "outputs": [],
      "source": [
        "def model_3():\n",
        "  model = keras.models.Sequential()\n",
        "  input = keras.layers.Input(shape=(13,))\n",
        "  model.add(input)\n",
        "  model.add(keras.layers.Dense(128, activation='tanh'))\n",
        "  model.add(keras.layers.Dense(64, activation='tanh'))\n",
        "  # NOTE: No activation function is used for the output layer because this is a regression,\n",
        "  # and we are interested in predicting numerical values directly without transformation\n",
        "  model.add(keras.layers.Dense(1))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_score_model_3 = kfold_history_score(model_3, X_train_scaled, y_train_scaled, epochs=100, n_splits=3)\n"
      ],
      "metadata": {
        "id": "135cyxgunQvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history_score_model_3)\n"
      ],
      "metadata": {
        "id": "QWC8kFFT4IvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_score_model_3_5_splits = kfold_history_score(model_1, X_train_scaled, y_train_scaled, epochs=100, n_splits=5)\n"
      ],
      "metadata": {
        "id": "KZ35_z-jR2hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history_score_model_3_5_splits)\n"
      ],
      "metadata": {
        "id": "HpwOwgXDR9nA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWXgQ1RzzPf7"
      },
      "source": [
        "## d) Model 4 [2 points]\n",
        "\n",
        "- Same as Model 2, but use the rmsprop optimizer when training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgtE3hhxN2oj"
      },
      "outputs": [],
      "source": [
        "def model_4():\n",
        "  model = keras.models.Sequential()\n",
        "  input = keras.layers.Input(shape=(13,))\n",
        "  model.add(input)\n",
        "  model.add(keras.layers.Dense(128, activation='relu'))\n",
        "  model.add(keras.layers.Dense(64, activation='relu'))\n",
        "  # NOTE: No activation function is used for the output layer because this is a regression,\n",
        "  # and we are interested in predicting numerical values directly without transformation\n",
        "  model.add(keras.layers.Dense(1))\n",
        "\n",
        "  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_score_model_4 = kfold_history_score(model_4, X_train_scaled, y_train_scaled, epochs=100, n_splits=3)\n"
      ],
      "metadata": {
        "id": "xG-9y-VlnGEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history_score_model_4)\n"
      ],
      "metadata": {
        "id": "T1knhVX-4iIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_score_model_4_5_splits = kfold_history_score(model_1, X_train_scaled, y_train_scaled, epochs=100, n_splits=5)\n"
      ],
      "metadata": {
        "id": "Ilba7BDbR3Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history_score_model_4_5_splits)\n"
      ],
      "metadata": {
        "id": "_xVSUxYZR_iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_d0KCd-zdcb"
      },
      "source": [
        "## e) Model Comparison [1 point]\n",
        "\n",
        "Which model performed best? Offer your thoughts on why the particular choice of hyperparameters led to improved performance for this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBKbdEwR0EcD"
      },
      "outputs": [],
      "source": [
        "# explain WHY you think the best model was better than the rest, in terms\n",
        "# of how those hyperparameters theoretically impact the model\n",
        "\n",
        "# provide visualizations (e.g. tables or comparison plot) to support your response where possible"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CONCLUSION\n",
        "\n",
        "- With 64 neurons in both layers, Model 1 is less complex and has fewer parameters than Model 2, 3 and 4. This also means that each one of Models 2, 3 and 4 has a higher capacity to learn complex patterns in the data.\n",
        "\n",
        "- Because Model 1 is simpler (compared to 2, 3 and 4), it is less likely to overfit the data, which also means that with more parameters, Models 2, 3 and 4 all have a higher risk of overfitting.\n",
        "\n",
        "- Fewer parameters will allow Model 1 to have quicker training time compared to Model 2, 3 and 4."
      ],
      "metadata": {
        "id": "M_VGnkfEO5kF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U2qAmO01FtC"
      },
      "source": [
        "NOTE: 2 additional points are awarded based on code documentation and overall clarity of work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a25KyERpPjC0"
      },
      "outputs": [],
      "source": [
        "# We are looking for a clear explanation of results with each response. We want you to attempt to\n",
        "# explain the _how_ and _why_ behind your answers, and not just the what, do demonstrate\n",
        "# your knowledge of the concepts discussed in class. Answers should be backed up with\n",
        "# visualizations (e.g. plots, charts).\n",
        "\n",
        "# Code should be easy to follow by using sensical naming conventions for function and variable\n",
        "# names, providing useful code comments, and refactoring repeated code into re-usable functions."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}